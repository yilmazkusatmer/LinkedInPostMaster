# ðŸš€ LinkedInPostMaster

AI-powered Streamlit application for optimizing LinkedIn posts based on 74 high-performing reference posts with optional o3-judge evaluation.

## Features

- Drei Optimierungsvarianten pro Modell (emotional, storytelling, lokal) ohne Emojis und mit Erhalt aller Links.
- UnterstÃ¼tzte Modelle: `gpt-4o`, `gpt-4.1`, `gpt-5` (via `responses.create`).
- Parallelmodus zur gleichzeitigen AusfÃ¼hrung aller Modelle.
- Optionale Richter:innenfunktion (`o3`), die auf Basis von Post-Beispielen den besten Vorschlag auswÃ¤hlt.
- Regelbasierte Voranalyse, die identifizierte StÃ¤rken und SchwÃ¤chen hervorhebt.
- Scrollbare, farbkodierte ErgebnisblÃ¶cke fÃ¼r verbesserte Lesbarkeit.
- Manuelle Eingabe des OpenAI-API-SchlÃ¼ssels im UI, falls keine `.env` vorhanden ist.

## Projektstruktur

```
LinkedInPostMaster/
â”œâ”€â”€ app.py                      # Streamlit-Einstiegspunkt
â”œâ”€â”€ README.md                   # Project docs
â”œâ”€â”€ requirements.txt            # Minimale AbhÃ¤ngigkeiten (Streamlit, OpenAI, dotenv)
â”œâ”€â”€ posts.json                  # Historische Beispielposts (Quelle fÃ¼r Regeln/Prompts)
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ analysis/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ post_analysis.py    # Regelbasierte Analyse
    â”œâ”€â”€ config.py               # AppConfig (Modelle, Defaults)
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ post_examples.py    # High-/Medium-/Low-Performer Beispiele
    â”œâ”€â”€ domain/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ performance_rules.py# Performanceregeln
    â”œâ”€â”€ prompts/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ optimizer_prompt.py # Prompt-Erzeugung fÃ¼r Optimierer
    â”œâ”€â”€ services/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ judge_service.py    # o3-Judge
    â”‚   â””â”€â”€ llm_client.py       # OpenAI-Client & Parsing
    â””â”€â”€ ui/
        â”œâ”€â”€ __init__.py
        â””â”€â”€ components.py       # Streamlit-UI-Komponenten (Panels & Judge-Output)
```

## Setup

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### OpenAI API Key

**Lokal (empfohlen):**

1. Kopiere `.env.example` zu `.env`:
   ```bash
   cp .env.example .env
   ```

2. FÃ¼ge deinen API Key ein:
   ```bash
   OPENAI_API_KEY=sk-your-actual-api-key
   ```

**Alternative:** Manuell im rechten SeitenmenÃ¼ der App eingeben.

## App starten

```bash
streamlit run app.py
```

Die App Ã¶ffnet sich im Browser (Standard: http://localhost:8501).

## Deployment

### Streamlit Cloud (Empfohlen)

Siehe **[DEPLOYMENT.md](DEPLOYMENT.md)** fÃ¼r detaillierte Schritt-fÃ¼r-Schritt Anleitung.

**Quick Start:**
```bash
# 1. Code auf GitHub pushen
git add .
git commit -m "Initial deployment"
git push origin master

# 2. Streamlit Cloud Ã¶ffnen
# â†’ share.streamlit.io
# â†’ New app â†’ Repository auswÃ¤hlen
# â†’ Secrets konfigurieren: OPENAI_API_KEY
# â†’ Deploy!
```

**Wichtig:**
- âœ… `.env` wird NICHT committed (in `.gitignore`)
- âœ… API Key Ã¼ber Streamlit Secrets konfigurieren
- âœ… Automatisches Re-Deployment bei jedem Git Push

## Tests / Validierung

Das Projekt enthÃ¤lt automatisierte Unit-Tests fÃ¼r die KernfunktionalitÃ¤ten.

### Tests ausfÃ¼hren

```bash
# Alle Tests ausfÃ¼hren
./test_run.sh

# Mit ausfÃ¼hrlicher Ausgabe
./test_run.sh --verbose

# Mit Coverage-Report
./test_run.sh --coverage

# Spezifischen Test ausfÃ¼hren
./test_run.sh --specific test_llm_client.py
```

### Test-Struktur

```
tests/
â”œâ”€â”€ test_llm_client.py          # Tests fÃ¼r OpenAI Client & Response Parsing
â”œâ”€â”€ test_optimizer_prompt.py    # Tests fÃ¼r Prompt-Generierung
â””â”€â”€ test_post_analysis.py       # Tests fÃ¼r regelbasierte Analyse
```

### Manuelle Validierung

- Vor Deployments: Smoke-Test lokal durchfÃ¼hren (`streamlit run app.py`)
- API-Integration: Testen mit realem OpenAI API Key

## Bekannte Limitierungen

- Historische Datenbasis: nur 74 Posts â†’ eingeschrÃ¤nkte ReprÃ¤sentativitÃ¤t.
- o3-Judge kann nur mit passenden API-Limits verwendet werden.
- Es werden keine Kostenkontrollen fÃ¼r parallele API-Aufrufe vorgenommen.

## Weiterentwicklungsideen

- âœ… ~~Unit-Tests fÃ¼r Analyse- und Prompt-Logik~~ (implementiert)
- Integration-Tests fÃ¼r End-to-End-Workflows
- Konfigurierbare Regeln (z.B. via YAML/JSON im Backend)
- Persistenzschicht fÃ¼r Nutzer:innenprofile oder gespeicherte Optimierungen
- Kostenmonitoring bei intensiven LLM-Abfragen
- CI/CD Pipeline mit automatischen Tests

## Lizenz

Nicht angegeben. Bitte bei Bedarf ergÃ¤nzen.


